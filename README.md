# SKGVisionPapers
Collection of papers related to Visual Structured Knowledge Grouding

## Papers
1. **UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models.** 

    *Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, Tao Yu*  [[pdf](https://arxiv.org/abs/2201.05966)], 2022.1

2. **Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding.**

   *Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova* [[pdf](https://arxiv.org/abs/2210.03347)], 2022.10

3. **pix2code: Generating Code from a Graphical User Interface Screenshot.**

   *Tony Beltramelli* [[pdf](https://arxiv.org/abs/1705.07962)], 2017.5

4. **TableFormer: Table Structure Understanding with Transformers.**

   *Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar* [[pdf](https://arxiv.org/abs/2203.01017)], 2022.3

5. **A large-scale dataset for end-to-end table recognition in the wild.**

   *Fan Yang, Lei Hu, Xinwu Liu, Shuangping Huang, Zhenghui Gu* [[pdf](https://arxiv.org/abs/2303.14884)], 2023.3

6. **[[Donut]]** **OCR-free Document Understanding Transformer.**

   *Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park* [[pdf](https://arxiv.org/abs/2111.15664)], 2021.11

7. **ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning.**

   *Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, Enamul Hoque* [[pdf](https://arxiv.org/abs/2203.10244)], 2022.3

8. **SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs.**

   *Shengzhi Li, Nima Tajbakhsh* [[pdf](https://arxiv.org/abs/2308.03349)], 2023.8

9. **FigureQA: An Annotated Figure Dataset for Visual Reasoning.**

    *Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, Yoshua Bengio* [[pdf](https://arxiv.org/abs/1710.07300)], 2017.10

10. **PlotQA: Reasoning over Scientific Plots.**

    *Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, Pratyush Kumar* [[pdf](https://arxiv.org/abs/1909.00997)], 2019.9

11. **MultiModalQA: Complex Question Answering over Text, Tables and Images.**

    *Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Jonathan Berant* [[pdf](https://arxiv.org/abs/2104.06039)], 2021.4

12. **ManyModalQA: Modality Disambiguation and QA over Diverse Inputs.**

    *Darryl Hannan, Akshay Jain, Mohit Bansal* [[pdf](https://arxiv.org/abs/2001.08034)], 2020.1

13. **LayoutLM: Pre-training of Text and Layout for Document Image Understanding.**

    *Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou* [[pdf](https://arxiv.org/abs/1912.13318)], 2019.12

14. **Parsing Table Structures in the Wild.**

    *Rujiao Long, Wen Wang, Nan Xue, Feiyu Gao, Zhibo Yang, Yongpan Wang, Gui-Song Xia* [[pdf](https://arxiv.org/abs/2109.02199)], 2021.9

15. **Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling.**

    *Yongshuai Huang, Ning Lu, Dapeng Chen, Yibo Li, Zecheng Xie, Shenggao Zhu, Liangcai Gao, Wei Peng* [[pdf](https://arxiv.org/abs/2303.06949)], 2023.3

16. **TabFact: A Large-scale Dataset for Table-based Fact Verification.**

    *Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang Wang* [[pdf](https://arxiv.org/abs/1909.02164)], 2019.9

17. **DocVQA: A Dataset for VQA on Document Images.**

    *Minesh Mathew, Dimosthenis Karatzas, C.V. Jawahar* [[pdf](https://arxiv.org/abs/2007.00398)], 2020.7

18. **InfographicVQA.**

    *Minesh Mathew, Viraj Bagal, Rubèn Pérez Tito, Dimosthenis Karatzas, Ernest Valveny, C.V Jawahar* [[pdf](https://arxiv.org/abs/2104.12756)], 2021.4
